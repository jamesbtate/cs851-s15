grep -v " 0 " wc_boilerpipe | awk4 | grep -o "\./tweets/[0-9]\+/" > boilerpipe_success_dirs

mkdir tweets2

for i in $(cat boilerpipe_success_dirs); do id=$(echo $i | grep -o "[0-9]\+"); mkdir tweets2/$id; cp "${i}url.0" tweets2/$id/url.0; done

# edit this script to use new directory and download2.stats
vim dereference_URIs.py

jtate@sirius:~/cs751/hw$ ./dereference_URIs.py
started: 3035  succeeded: 2889  header errors: 26  content errors: 120  elapsed seconds: 136

find ./tweets2/ > tweets2_file_list

./run_boilerpipe.py

wc tweets2/*/*/boilerpipe.output | tee wc_boilerpipe2

grep -v "^ \+0 " wc_boilerpipe | grep -v " total$" | awk4 | grep -o "56[0-9]\+" > boilerpipe1_ids
grep -v "^ \+0 " wc_boilerpipe2 | grep -v " total$" | awk4 | grep -o "56[0-9]\+" > boilerpipe2_ids

comm -12 <(sort boilerpipe1_ids) <(sort boilerpipe2_ids) > boilerpipe_common_ids

./jaccard.py | tee hw4_report/stats/q1_distances.stats
awk2 q1_distances.stats | tail -n +2 | sort -n > q1_unigrams.stats
awk3 q1_distances.stats | tail -n +2 | sort -n > q1_bigrams.stats
awk4 q1_distances.stats | tail -n +2 | sort -n > q1_trigrams.stats

#Q2:
./get_timemaps.py > get_timemap_output

#I had to go back and separately download the timemaps for 20 mementos that did not download properly
#due to bad final URI in tweets.summary.json. not gonna put this part in report because it would take
# forever to explain

# Get list of all mementos for timemaps
grep "\.timemap$" tweets2_file_list | xargs grep memento | grep -v rel=\"timemap\" | grep -v rel=\"timegate\" | grep -v rel=\"self\" | less | tee memento_list

./count_mementos.py > memento_counts
for i in $(seq -f "%04g" 1 1715); do echo "padding_$i 0"; done >> memento_counts
awk2 memento_counts | sort -n > hw4_report/stats/memento_counts.stats

./count_mementos.py | sort -nrk 2 | awk '{if ($2 > 19) print $1}' > twenty-plus_mementos
# used python interpreter to save list of tweet ids where first URI is older than two years:
>>> import json
>>> summaryFile = open('tweets.summary.json', 'r')
>>> summary = json.load(summaryFile)
>>> old = [tweet for tweet in summary['tweets'] if tweet['urls'][0]['timeDelta'] > 0.0]
>>> len(old)
5767
>>> old = [tweet for tweet in summary['tweets'] if tweet['urls'][0]['timeDelta'] > 86400.0]
>>> len(old)
3160
>>> old = [tweet for tweet in summary['tweets'] if tweet['urls'][0]['timeDelta'] > 365*86400.0]
>>> len(old)
600
>>> old = [tweet for tweet in summary['tweets'] if tweet['urls'][0]['timeDelta'] > 730*86400.0]
>>> len(old)
373
>>> old_ids = [tweet['id'] for tweet in old]
>>> a = open('two-year-old_tweets', 'w')
>>> for id in old_ids:
...     a.write(id + '\n')
...
>>> a.close()
# find old URIs that also have 20+ mementos
comm -12 <(sort two-year-old_ids) <(sort twenty-plus_mementos) > old_with_20_ids

