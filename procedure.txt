Downlaoding Tweets:
./download_tweets.py

Culling Bad Tweets:
grep -vi porn output.log | grep

Extracting Tweet Info to Individual Directory per Tweet:
./extract_tweets.py output.log
    This will skip tweets that do not have any URIs.

Running curl and wget:
./dereference_URIs.py
    This will save three files for each URL:
        headers.X   the curl output
        content.X/wget.output   the wget stdout/stderr
        content.X/[webpage]     the content of the URL
    This also saves the download.stats file which contains a list of tweets that failed in curl or wget.

Generating Tweet Summary:
./summary.py -l tweets -p > tweets.summary.json

Removing Tweets with Errors:
./summary.py -r tweets.summary.json -d download.stats -x > tweets.summary.json.temp
mv tweets.summary.json.temp tweets.summary.json

Getting Carbon Date Info:
./summary.py -p tweets.summary.json -f
    This saves final URLs to files in tweet directories
./carbondate_URIs.py tweets/
    This saves carbon date info to files in tweet directories
./summary.py -l tweets -p > tweets.summary.json
    This re-reads the tweet directories and generates a summary, this time including carbon date info.

Printing Stats:

