Downlaoding Tweets:
./download_tweets.py

Culling Bad Tweets:
grep -vi porn output.log | grep

Extracting Tweet Info to Individual Directory per Tweet:
./extract_tweets.py output.log
    This will skip tweets that do not have any URIs.

Running curl and wget:
./dereference_URIs.py
    This will save three files for each URL:
        headers.X   the curl output
        content.X/wget.output   the wget stdout/stderr
        content.X/[webpage]     the content of the URL
    This also saves the download.stats file which contains a list of tweets that failed in curl or wget.

Generating Tweet Summary:
./summary.py -l tweets -p -t output_2015-01-31b > tweets.summary.json

Removing Tweets with Errors:
./summary.py -r tweets.summary.json -d download.stats -x > tweets.summary.json.temp
mv tweets.summary.json.temp tweets.summary.json

Getting Carbon Date Info:
./carbondate_URIs.py tweets.summary.json > tweets.summary.with_delta.json
    This saves the time delta (Topsy - earliest) to summary, then prints the summary back out to stdout.

Printing Stats:
./summary.py -r tweets.summary.with_delta.json -S
